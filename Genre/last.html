<!DOCTYPE html>

<html>

    <head>
        <title>Sarangi Papers - Genre</title>
        <style>
            #container{
            }

            .paper{
                    border: 1px solid black;
                    font-family: verdana;
                    font-size: 1.0em;
                    white-space: pre-wrap
            } 
            .paper b{
                    font-size: 0.8em;
            } 

            </style>
    </head>
            
    <body>
    <div id="container">
            <div class="paper" id="g11">
                    <b>Paper Code:</b> G11
                    <b>Title:</b> A Machine Learning Approach to Automatic Music Genre Classification 
                    <b>Author(s):</b> Carlos N. Silla Jr., Alessandro L. Koerich and Celso A. A. Kaestner 
                    <b>Date:</b> - 
                    <b>Pages:</b> 12
                    <b>Data Source:</b> Latin Music database
                    <b>Algorithms used:</b> Decision Trees, Naive-Bayes, Support Vector Machine, Multi-layer perceptron neuron nets and k Nearest Neighbours 
                    <b>Formality rating:</b> 4/5
                    <b>Interest rating:</b> 3/5
                    <b>Genre:</b> 10 different Latin genre


                    <b>Intro notes:</b> 
                        - Space decomposition done by merging of results of binary classifiers to produce final genre label.
                        - Time decomposition done by diving music according to time segments.
                        - Most important features for classification vary according to their origin in the music signal.

                    <b>Body notes:</b>
                        - Original multi-class problem is decomposed to series of binary classification problem, where most of the known classifers work better and merge to obtain final result.
                        - For Space decomposition, one-against-all(OAA) and round robin(RR) is used.
                        - Time decomposition is done into three segments- beginning, middle and end thus giving three feature sets.
                        - Time decomposition give k classification result and space dimenstion for a M-class problem produces M(OAA) or M(M-1)/2 results.
                        - For RR approach majority voting is used and rule based on the posteriori probability of the employed classifier for OAA to merge result.  
                        - For time decomposition majority vote rule is used for combining result.

            </div>
            <div class="paper" id="g12">
                    <b>Paper Code:</b> G12
                    <b>Title:</b> Musical genre classification
                    <b>Author(s):</b> Robert Neumayer
                    <b>Date:</b> -
                    <b>Pages:</b> 15
                    <b>Data Source:</b> -
                    <b>Algorithms used:</b> Multilayer perceptron
                    <b>Formality rating:</b> 3/5
                    <b>Interest rating:</b> 3/5
                    <b>Genre:</b> Metal, Punk Rock, Grunge and Hip-Hop


                    <b>Intro notes:</b>
                        - Discusses the similarities and differences between different genres according to a specific set of features.      

                    <b>Body notes:</b>
                        - Network needs anout ten times more training samples than its number of weights.
                        - 8 hidden unit gave highest accuracy.
                        - The amount of training data available also played key factor in accuracy.
                        - Selection of feature set also played key part in determining accuracy.

            </div>
            <div class="paper" id="g13">
                    <b>Paper Code:</b> G13
                    <b>Title:</b> Music Genre Classification using MFCC, SVM and BPNN
                    <b>Author(s):</b> Gursimran Kour and Neha Mehan
                    <b>Date:</b> February, 2015
                    <b>Pages:</b> 3
                    <b>Data Source:</b> Sorsa(2006)
                    <b>Algorithms used:</b> Back Propagation Neural Net and Support Vector Machine
                    <b>Formality rating:</b> 4/5
                    <b>Interest rating:</b> 4/5
                    <b>Genre:</b> -


                    <b>Intro notes:</b>
                        -Genre classification used for:-
                            a)Developing automatic playlists on MP3
                            b)Storing enormous number of songs online
                        -Classification based on MFCC

                    <b>Body notes:</b>
                        -Psychophysical studies have found the phenomena of the mel pitch scale and the critical band, and the frequency scale-warping to the mel scale has led to the cepstrum domain representation.
                        -BPNN got more accuracy than SVM.

            </div>
            <div class="paper" id="g14">
                    <b>Paper Code:</b> G14
                    <b>Title:</b> Automated Music Genre Classification
                    <b>Author(s):</b> Archit Rathore and  Margaux Dorido
                    <b>Date:</b> March 16, 2015
                    <b>Pages:</b> 2
                    <b>Data Source:</b> GTZAN genre dataset
                    <b>Algorithms used:</b> Support Vector Machine
                    <b>Formality rating:</b> 1/5
                    <b>Interest rating:</b> 1/5
                    <b>Genre:</b> Blues, Classical, Metal, Rock, Pop and Reggae


                    <b>Intro notes:</b>
                        -Mel-frequency Cepstrum(MFC) features used for representing sound based feature
                        
                    <b>Body notes:</b>
                        -MFC coefficients represent a set of short term power spectrum characteristics of the sound and have been used in the state-of-the-art recognition and sound categorisation techniques.
                        -Dataset consisted of music samples recorded under a large number of environments which is believe to aid in achievement of noise robust classifier for genres.

            </div>
            <div class="paper" id="g15">
                    <b>Paper Code:</b> G15
                    <b>Title:</b> A Comparative Study on Content-Based Music Genre Classification
                    <b>Author(s):</b> Tao Li, Mitsunori Ogihara and Qi Li
                    <b>Date:</b> -
                    <b>Pages:</b> 8
                    <b>Data Source:</b> 
                        -For Dataset A:radio, compact disks and MP3 compresses audio files 
                        -For Dataset B:CD collection
                    <b>Algorithms used:</b> Linear Discriminant Analysis, Support Vector Machine,Gaussian Mixture Models, K-Nearest Neighbors, Regression and decision trees including C4.5 and CART
                    <b>Formality rating:</b> 4/5
                    <b>Interest rating:</b> 4/5
                    <b>Genre:</b>
                        -For Dataset A:Blues, Classical, Country, Disco, Hiphop, Jazz, Metal, Pop, Reggae and Rock 
                        -For Dataset B:Ambient, Classical, Fusion, Jazz and Rock


                    <b>Intro notes:</b>
                        -Proposes new feature extraction method for music genre classification DWCHs(Daubechies Wavelet Coefficient Histograms).
                        -DWCHs capture the local and global information of music signals simultaneously by computing histograms on their Daubechies wavelet coefficients. 

                    <b>Body notes:</b>
                        -Timbral features capture the statistics of local information of music signals from a global perspective, but not enough in representing the global information of the music.
                        -Three different methods to extend SVM for multi-class: pairwise, one-against-the-rest and multi-class objective function.
                        -For Gaussian Mixture Models, three Gaussian mixtures to model each genre.
                        -DWCHs significantly improved the classification accuracy.

            </div>
            <div class="paper" id="g16">
                    <b>Paper Code:</b> G16
                    <b>Title:</b> Musical Genre Classfication of Audio Signals
                    <b>Author(s):</b> George Tzanetakis and Perry Cook
                    <b>Date:</b> July, 2002
                    <b>Pages:</b> 10
                    <b>Data Source:</b> Radio, compact disks and Mp3 compressed audio files
                    <b>Algorithm used:</b> Gaussian mixture model classifier, K-means, K-Nearest Neighbor
                    <b>Formality rating:</b> 5/5
                    <b>Interest rating:</b> 4/5
                    <b>Genre:</b> 
                        -Genre dataset:classical, country, disco, hiphop, jazz, rock, blues,reggae, pop and metal
                        -Classical dataset:choir, orchestra, piano and string quartet
                        -Jazz dataset:bigband, cool, fusion, piano, quartet and swing


                    <b>Intro notes:</b>
                        -Both whole file and real-time frame-based classification schemes are used.
                        -Timbral texture feature set is based on features used for speech and general sound classification, the other two feature sets (rhythmic and pitch content) are new and specifically designed to represent aspects of musical content (rhythm and harmony).

                    <b>Body notes:</b>
                        -Rhythmic and pitch content feature set can be computed over whole file if the the file is relatively homogeneous.
                        -Automatic segmentation algorithms can be used to segment the file into regions and apply classification to each region separately.
                        -Frames from the same audio file are never split between training and testing data in order to avoid false higher accuracy due to the similarity of feature vectors from the same file.
                        -Process is iterated with different random partitions and the results are averaged to ensure that the calculated accuracy will not be biased because of a particular partitioning of training and testing.
                        -Change of texture window size affected he classification accuracy.
                        -From use of individual feature set for task of automatic musical genre classification, it was seen that the nontimbral texture features pitch histogram features (PHF) and beat histogram features (BHF) performed worse than the timbral-texture features (STFT, MFCC) in all cases.
                    
            </div>
            <div class="paper" id="g17">
                    <b>Paper Code:</b> G17
                    <b>Title:</b> Automatic Musical Genre Classification Of Audio Signals
                    <b>Author(s)</b> George Tzanetakis, Georg Essl and Perry Cook
                    <b>Date:</b> -
                    <b>Pages:</b> 6
                    <b>Data Source:</b> Compact disks, radio and web
                    <b>Algorithm used:</b> Gaussian classifier
                    <b>Formality rating:</b> 5/5
                    <b>Interest rating:</b> 4/5
                    <b>Genre:</b>
                        -Genre dataset:classical, country, disco, hiphop, jazz and rock
                        -Classical dataset:choral, orchestral, piano and string 4tet


                    <b>Intro notes:</b>
                        -Audio signals can be automatically classfied using a hierarchy of genres that can be represented as a tree with 15 nodes.

                    <b>Body notes:</b>
                        -Musical surface features denote the characteristics of music related to texture, timbre and instrumentation and for its representation 9-dimensional feature vector is used which are:mean-Centroid, mean-Rolloff, mean-Flux, mean-ZeroCrossings, std-Centroid, std-Rolloff, std-Flux, std-ZeroCrossings and LowEnergy.
                        -Rhythmic feature set is based on detecting the most salient periodicities of the signal.

            </div>

    </div>

</html>

